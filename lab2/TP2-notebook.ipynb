{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 2 - 3 Hours </h1>\n",
    "<h1 style=\"text-align:center\"> Convolutional Neural Network (CNN) for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b> Student 1:</b> Daniele Reda  \n",
    "<br>\n",
    "<b> Student 2:</b> Matteo Romiti\n",
    "</div> \n",
    " \n",
    "The aim of this session is to practice with Convolutional Neural Networks. Answers and experiments should be made by groups of one or two students. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "\n",
    "Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an pdf document using print as PDF (Ctrl+P). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed by May 29th 2017.\n",
    "\n",
    "Send you pdf file to benoit.huet@eurecom.fr and olfa.ben-ahmed@eurecom.fr using **[DeepLearning_lab2]** as Subject of your email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last Lab Session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%.  Can  you do better than these results using a deep CNN ?\n",
    "In this Lab Session, you will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks:  **LeNet-5** to go to  more than 99% of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data in TensorFlow\n",
    "Run the cell above to load the MNIST data that comes  with TensorFlow. You will use this data in **Section 1** and **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape:    (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "print(\"Image Shape:    {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : My First Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow the example :\n",
    "**y=softmax(Wx+b)** seen in the DeepLearing course last week. \n",
    "\n",
    "This model reaches an accuracy of about 92 %.\n",
    "You will also learn how to launch the tensorBoard https://www.tensorflow.org/get_started/summaries_and_tensorboard to  visualize the computation graph, statistics and learning curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : Read carefully the code in the cell below. Run it to perform training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  02   =====> Loss= 0.733064874\n",
      "Epoch:  04   =====> Loss= 0.536358523\n",
      "Epoch:  06   =====> Loss= 0.471889261\n",
      "Epoch:  08   =====> Loss= 0.435504827\n",
      "Epoch:  10   =====> Loss= 0.412166757\n",
      "Epoch:  12   =====> Loss= 0.397256630\n",
      "Epoch:  14   =====> Loss= 0.384323012\n",
      "Epoch:  16   =====> Loss= 0.374909195\n",
      "Epoch:  18   =====> Loss= 0.365859221\n",
      "Epoch:  20   =====> Loss= 0.358819640\n",
      "Epoch:  22   =====> Loss= 0.353986612\n",
      "Epoch:  24   =====> Loss= 0.349985992\n",
      "Epoch:  26   =====> Loss= 0.345158857\n",
      "Epoch:  28   =====> Loss= 0.340375543\n",
      "Epoch:  30   =====> Loss= 0.337398454\n",
      "Epoch:  32   =====> Loss= 0.333335278\n",
      "Epoch:  34   =====> Loss= 0.330760923\n",
      "Epoch:  36   =====> Loss= 0.326252774\n",
      "Epoch:  38   =====> Loss= 0.325419832\n",
      "Epoch:  40   =====> Loss= 0.322225989\n",
      "Epoch:  42   =====> Loss= 0.320298648\n",
      "Epoch:  44   =====> Loss= 0.319281198\n",
      "Epoch:  46   =====> Loss= 0.318291789\n",
      "Epoch:  48   =====> Loss= 0.314709458\n",
      "Epoch:  50   =====> Loss= 0.313845383\n",
      "Epoch:  52   =====> Loss= 0.309665868\n",
      "Epoch:  54   =====> Loss= 0.311834381\n",
      "Epoch:  56   =====> Loss= 0.306622827\n",
      "Epoch:  58   =====> Loss= 0.308776800\n",
      "Epoch:  60   =====> Loss= 0.305952971\n",
      "Epoch:  62   =====> Loss= 0.303234257\n",
      "Epoch:  64   =====> Loss= 0.304481421\n",
      "Epoch:  66   =====> Loss= 0.302969264\n",
      "Epoch:  68   =====> Loss= 0.304459532\n",
      "Epoch:  70   =====> Loss= 0.303972692\n",
      "Epoch:  72   =====> Loss= 0.298687273\n",
      "Epoch:  74   =====> Loss= 0.300435235\n",
      "Epoch:  76   =====> Loss= 0.297799206\n",
      "Epoch:  78   =====> Loss= 0.299489916\n",
      "Epoch:  80   =====> Loss= 0.295029076\n",
      "Epoch:  82   =====> Loss= 0.297068079\n",
      "Epoch:  84   =====> Loss= 0.296208043\n",
      "Epoch:  86   =====> Loss= 0.293802273\n",
      "Epoch:  88   =====> Loss= 0.290398892\n",
      "Epoch:  90   =====> Loss= 0.294975000\n",
      "Epoch:  92   =====> Loss= 0.291894806\n",
      "Epoch:  94   =====> Loss= 0.292634473\n",
      "Epoch:  96   =====> Loss= 0.292533482\n",
      "Epoch:  98   =====> Loss= 0.290901574\n",
      "Epoch:  100   =====> Loss= 0.289058641\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9203\n"
     ]
    }
   ],
   "source": [
    "#STEP 1\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 128\n",
    "display_step = 2\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "#STEP 2 \n",
    "\n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2  </b>: Using Tensorboard, we can  now visualize the created graph, giving you an overview of your architecture and how all of the major components  are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch tensorBoard: \n",
    "- Go to the **TP2** folder, \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir= log_files/\"**, it will generate an http link ,ex http://666.6.6.6:6006,\n",
    "- Copy this  link into your web browser \n",
    "\n",
    "\n",
    "Enjoy It !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : The 99% MNIST Challenge !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : LeNet5 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are familar with **tensorFlow** and **tensorBoard**, you are in this section to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "In more advanced step you will make some optimizations to get more than 99% of accuracy. The best model can get to over 99.7% accuracy! \n",
    "\n",
    "For more information, have a look at this list of results : http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"lenet.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 1: Lenet 5 </span></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1: Convolutional.** The output shape should be 28x28x6 **Activation.** sigmoid **Pooling.** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 10x10x16. **Activation.** sigmoid **Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use **flatten*  from tensorflow.contrib.layers import flatten\n",
    "\n",
    "**Layer 3: Fully Connected.** This should have 120 outputs. **Activation.** sigmoid\n",
    "\n",
    "**Layer 4: Fully Connected.** This should have 84 outputs. **Activation.** sigmoid\n",
    "\n",
    "**Layer 5: Fully Connected.** This should have 10 outputs **Activation.** softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.1 </b>  Implement the Neural Network architecture described above.\n",
    "For that, your will use classes and functions from  https://www.tensorflow.org/api_docs/python/tf/nn. \n",
    "\n",
    "We give you some helper functions for weigths and bias initilization. Also you can refer to section 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions  for weigths and bias initilization \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride, padding_):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5_Model(data, transfer=\"sigmoid\", keep_prob=1.):    \n",
    "    # your implementation goes here\n",
    "    \n",
    "    transferFuncs = {\"sigmoid\" : tf.sigmoid, \"ReLU\": tf.nn.relu}\n",
    "        \n",
    "    #first convolutional layer\n",
    "    W_conv1 = weight_variable([5, 5, 1, 6]) ## [filter_width, filter_height, depth_image_in, depth_image_out]\n",
    "    b_conv1 = bias_variable([6])\n",
    "    h_conv1 = transferFuncs[transfer](conv2d(data, W_conv1, 1, 'SAME') + b_conv1)\n",
    "    pool1 = tf.nn.pool(h_conv1, [2,2], \"MAX\", 'VALID', strides=[2,2])\n",
    "    \n",
    "    #second convolutional layer\n",
    "    W_conv2 = weight_variable([5, 5, 6, 16])\n",
    "    b_conv2 = bias_variable([16])\n",
    "    h_conv2 = transferFuncs[transfer](conv2d(pool1, W_conv2, 1, 'VALID') + b_conv2)\n",
    "    pool2 = tf.nn.pool(h_conv2, [2,2], \"MAX\", 'VALID', strides=[2,2])\n",
    "    \n",
    "    #first fully connected layer\n",
    "    s = pool2.get_shape().as_list()\n",
    "    flattened_length = s[1] * s[2] * s[3]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, flattened_length])\n",
    "    W_fc1 = weight_variable([1*5*5*16, 120])\n",
    "    b_fc1 = bias_variable([120])\n",
    "    h_fc1 = transferFuncs[transfer](tf.matmul(pool2_flat, W_fc1) + b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    #second fully connected layer\n",
    "    W_fc2 = weight_variable([120, 84])\n",
    "    b_fc2 = bias_variable([84])\n",
    "    h_fc2 = transferFuncs[transfer](tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "    \n",
    "    #third fully connected layer\n",
    "    W_fc3 = weight_variable([84, 10])\n",
    "    b_fc3 = bias_variable([10])\n",
    "    h_fc3 = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "    \n",
    "    return h_fc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.2. </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59706\n"
     ]
    }
   ],
   "source": [
    "# first conv\n",
    "pconv1 = 5*5*1*6 # filter_height * filter_width * channels_in * num_feature_maps\n",
    "# second conv\n",
    "pconv2 = 5*5*1*16 # filter_height * filter_width * channels_in * num_feature_maps\n",
    "# first fcl\n",
    "pfcl1 = 5*5*16*120 # fcl_input_size * fcl_output_size\n",
    "pfcl1# second fcl\n",
    "pfcl2 = 84*120 # fcl_input_size * fcl_output_size\n",
    "# third fcl\n",
    "pfcl3 = 84*10 # fcl_input_size * fcl_output_size\n",
    "pbias = 6+16+120+84+10 # all the biases\n",
    "total = pbias + pfcl1 + pfcl2 + pfcl3 + pconv2 + pconv1\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.3. </b>  Start the training with the parameters cited below:\n",
    "\n",
    "     Learning rate : 0.1\n",
    "     Loss Function : Cross entropy\n",
    "     Optimizer: SGD\n",
    "     Number of training iterations : 10000\n",
    "     Batch size : 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "training_epochs = 200 # as suggested in the email\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "logs_path = 'log_files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, y):\n",
    "    correct = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "    return tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.5. </b>  Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(learning_rate, training_epochs, batch_size, display_step = 1, \\\n",
    "          logs_path='log_files/', optFunction=\"SGD\", verbose=True, transfer=\"sigmoid\", keep_probability= 1.0):\n",
    "    \n",
    "    optFunctions = {\"SGD\":tf.train.GradientDescentOptimizer, \"Adam\":tf.train.AdamOptimizer}\n",
    "    \n",
    "    # Erase previous graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 28, 28, 1], name='InputData')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Construct model\n",
    "    with tf.name_scope('Model'):\n",
    "        pred = LeNet5_Model(x, transfer=transfer)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    with tf.name_scope('Loss'):\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "        #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "    with tf.name_scope(optFunction):\n",
    "        if transfer is \"sigmoid\":\n",
    "            optimizer = optFunctions[optFunction](learning_rate).minimize(cost)\n",
    "        else:\n",
    "            opt = optFunctions[optFunction](learning_rate)\n",
    "            gvs = opt.compute_gradients(cost)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "            optimizer = opt.apply_gradients(capped_gvs)\n",
    "\n",
    "    # Evaluate model\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        accuracy = evaluate(pred, y)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a summary to monitor cost tensor\n",
    "    tf.summary.scalar(\"Loss\", cost)\n",
    "    # Create a summary to monitor accuracy tensor\n",
    "    tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    x_val, y_val = mnist.validation.images.reshape(-1, 28, 28, 1), mnist.validation.labels\n",
    "    x_test, y_test = mnist.test.images.reshape(-1, 28, 28, 1), mnist.test.labels\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "#         acc_history = []\n",
    "        test_history = []\n",
    "        val_history = []\n",
    "#         train_history = []\n",
    "\n",
    "        sess.run(init)\n",
    "        if verbose is True:\n",
    "            print(\"Start Training!\")\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        saver = tf.train.Saver()\n",
    "        #Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            #Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "                batch_xs = batch_xs.reshape(-1, 28, 28, 1)\n",
    "                # Run optimization op (backprop), cost op (to get loss value)\n",
    "                # and summary nodes\n",
    "                _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                         feed_dict={x: batch_xs, y: batch_ys, keep_prob: keep_probability})\n",
    "#                 _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "#                                          feed_dict={x: batch_xs, y: batch_ys, keep_prob:keep_prob_})\n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "            # Display logs per epoch step\n",
    "#             train_acc = accuracy.eval({x: batch_xs, y:batch_ys})\n",
    "            val_acc = accuracy.eval({x: x_val, y:y_val, keep_prob:1.0})\n",
    "            test_acc = accuracy.eval({x: x_test, y:y_test, keep_prob:1.0})\n",
    "#             acc_history.append(acc)\n",
    "#             train_history.append(train_acc)\n",
    "            val_history.append(val_acc)\n",
    "            test_history.append(test_acc)\n",
    "            \n",
    "            saver.save(sess, 'Models/model_' + str(learning_rate) + '_' + str(batch_size) + '_' + optFunction)\n",
    "            if verbose is True and (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \\\n",
    "                      \"  =====> Loss=\", \"{:.9f}\".format(avg_cost), \\\n",
    "                      \" Validation accuracy=\", val_acc, \" Test accuracy=\", test_acc)\n",
    "            if val_acc>=0.99:\n",
    "                if verbose is True:\n",
    "                    print(\"Validation Accuracy over 99%% reached after %d epochs\" %(epoch+1))\n",
    "                break\n",
    "                \n",
    "        if verbose is True:\n",
    "            print(\"Training Finished!\")\n",
    "            # Test model\n",
    "            # Calculate accuracy\n",
    "            print(\"Test accuracy:\", accuracy.eval({x: x_test, y:y_test, keep_prob:1.0}))\n",
    "        \n",
    "    return val_history, test_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train.next_batch()` has shuffle parameter set to True by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "Epoch:  01   =====> Loss= 2.306711101  Validation accuracy= 0.1126  Test accuracy= 0.1135\n",
      "Epoch:  02   =====> Loss= 2.305636428  Validation accuracy= 0.1126  Test accuracy= 0.1135\n",
      "Epoch:  03   =====> Loss= 2.304683939  Validation accuracy= 0.099  Test accuracy= 0.1009\n",
      "Epoch:  04   =====> Loss= 2.304435188  Validation accuracy= 0.0986  Test accuracy= 0.101\n",
      "Epoch:  05   =====> Loss= 2.302418652  Validation accuracy= 0.1126  Test accuracy= 0.1135\n",
      "Epoch:  06   =====> Loss= 2.300473815  Validation accuracy= 0.0868  Test accuracy= 0.0892\n",
      "Epoch:  07   =====> Loss= 2.294925442  Validation accuracy= 0.1582  Test accuracy= 0.1736\n",
      "Epoch:  08   =====> Loss= 2.272766478  Validation accuracy= 0.2568  Test accuracy= 0.2669\n",
      "Epoch:  09   =====> Loss= 1.972761237  Validation accuracy= 0.5516  Test accuracy= 0.5554\n",
      "Epoch:  10   =====> Loss= 1.132301843  Validation accuracy= 0.7514  Test accuracy= 0.7543\n",
      "Epoch:  11   =====> Loss= 0.731297972  Validation accuracy= 0.8326  Test accuracy= 0.8345\n",
      "Epoch:  12   =====> Loss= 0.539246344  Validation accuracy= 0.8642  Test accuracy= 0.8697\n",
      "Epoch:  13   =====> Loss= 0.425185097  Validation accuracy= 0.8948  Test accuracy= 0.8911\n",
      "Epoch:  14   =====> Loss= 0.347723823  Validation accuracy= 0.9178  Test accuracy= 0.9133\n",
      "Epoch:  15   =====> Loss= 0.296964134  Validation accuracy= 0.9294  Test accuracy= 0.9254\n",
      "Epoch:  16   =====> Loss= 0.254887692  Validation accuracy= 0.9402  Test accuracy= 0.9359\n",
      "Epoch:  17   =====> Loss= 0.227199897  Validation accuracy= 0.9464  Test accuracy= 0.9392\n",
      "Epoch:  18   =====> Loss= 0.201696811  Validation accuracy= 0.9488  Test accuracy= 0.9474\n",
      "Epoch:  19   =====> Loss= 0.183556086  Validation accuracy= 0.9548  Test accuracy= 0.9534\n",
      "Epoch:  20   =====> Loss= 0.167839546  Validation accuracy= 0.9588  Test accuracy= 0.9571\n",
      "Epoch:  21   =====> Loss= 0.156564979  Validation accuracy= 0.9612  Test accuracy= 0.9588\n",
      "Epoch:  22   =====> Loss= 0.144359405  Validation accuracy= 0.9646  Test accuracy= 0.9632\n",
      "Epoch:  23   =====> Loss= 0.132515675  Validation accuracy= 0.9664  Test accuracy= 0.9654\n",
      "Epoch:  24   =====> Loss= 0.127158645  Validation accuracy= 0.9676  Test accuracy= 0.967\n",
      "Epoch:  25   =====> Loss= 0.120277383  Validation accuracy= 0.9696  Test accuracy= 0.9693\n",
      "Epoch:  26   =====> Loss= 0.112007220  Validation accuracy= 0.97  Test accuracy= 0.9707\n",
      "Epoch:  27   =====> Loss= 0.109625914  Validation accuracy= 0.9704  Test accuracy= 0.9716\n",
      "Epoch:  28   =====> Loss= 0.103575878  Validation accuracy= 0.971  Test accuracy= 0.972\n",
      "Epoch:  29   =====> Loss= 0.098601557  Validation accuracy= 0.973  Test accuracy= 0.9739\n",
      "Epoch:  30   =====> Loss= 0.093816171  Validation accuracy= 0.974  Test accuracy= 0.9744\n",
      "Epoch:  31   =====> Loss= 0.092868912  Validation accuracy= 0.9726  Test accuracy= 0.9741\n",
      "Epoch:  32   =====> Loss= 0.089477438  Validation accuracy= 0.9738  Test accuracy= 0.9749\n",
      "Epoch:  33   =====> Loss= 0.084379114  Validation accuracy= 0.976  Test accuracy= 0.9769\n",
      "Epoch:  34   =====> Loss= 0.083824713  Validation accuracy= 0.9772  Test accuracy= 0.9784\n",
      "Epoch:  35   =====> Loss= 0.079959069  Validation accuracy= 0.9754  Test accuracy= 0.9791\n",
      "Epoch:  36   =====> Loss= 0.079306742  Validation accuracy= 0.9772  Test accuracy= 0.9762\n",
      "Epoch:  37   =====> Loss= 0.076031795  Validation accuracy= 0.9766  Test accuracy= 0.9787\n",
      "Epoch:  38   =====> Loss= 0.075798994  Validation accuracy= 0.9764  Test accuracy= 0.9804\n",
      "Epoch:  39   =====> Loss= 0.071249567  Validation accuracy= 0.976  Test accuracy= 0.9806\n",
      "Epoch:  40   =====> Loss= 0.071117505  Validation accuracy= 0.9784  Test accuracy= 0.9807\n",
      "Epoch:  41   =====> Loss= 0.068700804  Validation accuracy= 0.9788  Test accuracy= 0.9808\n",
      "Epoch:  42   =====> Loss= 0.068370116  Validation accuracy= 0.978  Test accuracy= 0.9817\n",
      "Epoch:  43   =====> Loss= 0.066696880  Validation accuracy= 0.9776  Test accuracy= 0.9817\n",
      "Epoch:  44   =====> Loss= 0.065036479  Validation accuracy= 0.9788  Test accuracy= 0.9818\n",
      "Epoch:  45   =====> Loss= 0.062617577  Validation accuracy= 0.9798  Test accuracy= 0.9827\n",
      "Epoch:  46   =====> Loss= 0.061715125  Validation accuracy= 0.98  Test accuracy= 0.9815\n",
      "Epoch:  47   =====> Loss= 0.062542747  Validation accuracy= 0.9806  Test accuracy= 0.9814\n",
      "Epoch:  48   =====> Loss= 0.058317994  Validation accuracy= 0.9822  Test accuracy= 0.9827\n",
      "Epoch:  49   =====> Loss= 0.061187078  Validation accuracy= 0.9804  Test accuracy= 0.9834\n",
      "Epoch:  50   =====> Loss= 0.057029585  Validation accuracy= 0.98  Test accuracy= 0.9812\n",
      "Epoch:  51   =====> Loss= 0.055684161  Validation accuracy= 0.9798  Test accuracy= 0.9822\n",
      "Epoch:  52   =====> Loss= 0.055113544  Validation accuracy= 0.9818  Test accuracy= 0.9833\n",
      "Epoch:  53   =====> Loss= 0.056086201  Validation accuracy= 0.9808  Test accuracy= 0.9834\n",
      "Epoch:  54   =====> Loss= 0.053827283  Validation accuracy= 0.9816  Test accuracy= 0.9838\n",
      "Epoch:  55   =====> Loss= 0.051751721  Validation accuracy= 0.9824  Test accuracy= 0.9839\n",
      "Epoch:  56   =====> Loss= 0.051553623  Validation accuracy= 0.982  Test accuracy= 0.9843\n",
      "Epoch:  57   =====> Loss= 0.052797941  Validation accuracy= 0.9828  Test accuracy= 0.9846\n",
      "Epoch:  58   =====> Loss= 0.048138580  Validation accuracy= 0.983  Test accuracy= 0.9839\n",
      "Epoch:  59   =====> Loss= 0.048249521  Validation accuracy= 0.9834  Test accuracy= 0.985\n",
      "Epoch:  60   =====> Loss= 0.048580303  Validation accuracy= 0.9846  Test accuracy= 0.9849\n",
      "Epoch:  61   =====> Loss= 0.047425265  Validation accuracy= 0.984  Test accuracy= 0.9862\n",
      "Epoch:  62   =====> Loss= 0.048077195  Validation accuracy= 0.9836  Test accuracy= 0.9852\n",
      "Epoch:  63   =====> Loss= 0.046083331  Validation accuracy= 0.9834  Test accuracy= 0.9853\n",
      "Epoch:  64   =====> Loss= 0.043985134  Validation accuracy= 0.9836  Test accuracy= 0.9851\n",
      "Epoch:  65   =====> Loss= 0.045851600  Validation accuracy= 0.9836  Test accuracy= 0.9838\n",
      "Epoch:  66   =====> Loss= 0.043868013  Validation accuracy= 0.9842  Test accuracy= 0.985\n",
      "Epoch:  67   =====> Loss= 0.043159742  Validation accuracy= 0.982  Test accuracy= 0.9845\n",
      "Epoch:  68   =====> Loss= 0.043726347  Validation accuracy= 0.9844  Test accuracy= 0.9862\n",
      "Epoch:  69   =====> Loss= 0.041593596  Validation accuracy= 0.9836  Test accuracy= 0.9856\n",
      "Epoch:  70   =====> Loss= 0.042528965  Validation accuracy= 0.9846  Test accuracy= 0.9854\n",
      "Epoch:  71   =====> Loss= 0.040201591  Validation accuracy= 0.9854  Test accuracy= 0.9858\n",
      "Epoch:  72   =====> Loss= 0.040041049  Validation accuracy= 0.9838  Test accuracy= 0.9848\n",
      "Epoch:  73   =====> Loss= 0.039296127  Validation accuracy= 0.985  Test accuracy= 0.9856\n",
      "Epoch:  74   =====> Loss= 0.039682834  Validation accuracy= 0.9846  Test accuracy= 0.9852\n",
      "Epoch:  75   =====> Loss= 0.037695736  Validation accuracy= 0.9848  Test accuracy= 0.9848\n",
      "Epoch:  76   =====> Loss= 0.038444788  Validation accuracy= 0.9846  Test accuracy= 0.9856\n",
      "Epoch:  77   =====> Loss= 0.037482553  Validation accuracy= 0.9864  Test accuracy= 0.987\n",
      "Epoch:  78   =====> Loss= 0.037332472  Validation accuracy= 0.9854  Test accuracy= 0.9867\n",
      "Epoch:  79   =====> Loss= 0.036921862  Validation accuracy= 0.9864  Test accuracy= 0.986\n",
      "Epoch:  80   =====> Loss= 0.036638310  Validation accuracy= 0.9852  Test accuracy= 0.9864\n",
      "Epoch:  81   =====> Loss= 0.034755491  Validation accuracy= 0.9868  Test accuracy= 0.9867\n",
      "Epoch:  82   =====> Loss= 0.036031529  Validation accuracy= 0.9862  Test accuracy= 0.9865\n",
      "Epoch:  83   =====> Loss= 0.034996243  Validation accuracy= 0.9854  Test accuracy= 0.9857\n",
      "Epoch:  84   =====> Loss= 0.032668294  Validation accuracy= 0.9874  Test accuracy= 0.9868\n",
      "Epoch:  85   =====> Loss= 0.035465714  Validation accuracy= 0.9864  Test accuracy= 0.9859\n",
      "Epoch:  86   =====> Loss= 0.032586588  Validation accuracy= 0.9874  Test accuracy= 0.9878\n",
      "Epoch:  87   =====> Loss= 0.033481765  Validation accuracy= 0.985  Test accuracy= 0.9864\n",
      "Epoch:  88   =====> Loss= 0.031420817  Validation accuracy= 0.9864  Test accuracy= 0.9863\n",
      "Epoch:  89   =====> Loss= 0.033429292  Validation accuracy= 0.9878  Test accuracy= 0.9878\n",
      "Epoch:  90   =====> Loss= 0.031617118  Validation accuracy= 0.9862  Test accuracy= 0.9859\n",
      "Epoch:  91   =====> Loss= 0.031943024  Validation accuracy= 0.9868  Test accuracy= 0.9872\n",
      "Epoch:  92   =====> Loss= 0.031059507  Validation accuracy= 0.9872  Test accuracy= 0.9863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  93   =====> Loss= 0.030869834  Validation accuracy= 0.9878  Test accuracy= 0.9871\n",
      "Epoch:  94   =====> Loss= 0.030361671  Validation accuracy= 0.9872  Test accuracy= 0.9877\n",
      "Epoch:  95   =====> Loss= 0.030616136  Validation accuracy= 0.987  Test accuracy= 0.9871\n",
      "Epoch:  96   =====> Loss= 0.028895149  Validation accuracy= 0.988  Test accuracy= 0.9874\n",
      "Epoch:  97   =====> Loss= 0.028924264  Validation accuracy= 0.9882  Test accuracy= 0.9877\n",
      "Epoch:  98   =====> Loss= 0.029672429  Validation accuracy= 0.9878  Test accuracy= 0.9879\n",
      "Epoch:  99   =====> Loss= 0.028289247  Validation accuracy= 0.9872  Test accuracy= 0.9868\n",
      "Epoch:  100   =====> Loss= 0.028785688  Validation accuracy= 0.9878  Test accuracy= 0.988\n",
      "Epoch:  101   =====> Loss= 0.027101249  Validation accuracy= 0.9876  Test accuracy= 0.9867\n",
      "Epoch:  102   =====> Loss= 0.027941262  Validation accuracy= 0.988  Test accuracy= 0.9878\n",
      "Epoch:  103   =====> Loss= 0.026916288  Validation accuracy= 0.9884  Test accuracy= 0.9884\n",
      "Epoch:  104   =====> Loss= 0.025974303  Validation accuracy= 0.9884  Test accuracy= 0.988\n",
      "Epoch:  105   =====> Loss= 0.028418892  Validation accuracy= 0.9866  Test accuracy= 0.9867\n",
      "Epoch:  106   =====> Loss= 0.026032649  Validation accuracy= 0.9874  Test accuracy= 0.9873\n",
      "Epoch:  107   =====> Loss= 0.026324572  Validation accuracy= 0.987  Test accuracy= 0.9868\n",
      "Epoch:  108   =====> Loss= 0.025792525  Validation accuracy= 0.9872  Test accuracy= 0.9877\n",
      "Epoch:  109   =====> Loss= 0.024758348  Validation accuracy= 0.9874  Test accuracy= 0.9878\n",
      "Epoch:  110   =====> Loss= 0.026273310  Validation accuracy= 0.988  Test accuracy= 0.9875\n",
      "Epoch:  111   =====> Loss= 0.024954320  Validation accuracy= 0.9868  Test accuracy= 0.9874\n",
      "Epoch:  112   =====> Loss= 0.024047538  Validation accuracy= 0.988  Test accuracy= 0.9878\n",
      "Epoch:  113   =====> Loss= 0.024775138  Validation accuracy= 0.9878  Test accuracy= 0.9883\n",
      "Epoch:  114   =====> Loss= 0.024530545  Validation accuracy= 0.9878  Test accuracy= 0.9885\n",
      "Epoch:  115   =====> Loss= 0.024088073  Validation accuracy= 0.987  Test accuracy= 0.988\n",
      "Epoch:  116   =====> Loss= 0.023213179  Validation accuracy= 0.988  Test accuracy= 0.9875\n",
      "Epoch:  117   =====> Loss= 0.023963250  Validation accuracy= 0.9878  Test accuracy= 0.988\n",
      "Epoch:  118   =====> Loss= 0.021890021  Validation accuracy= 0.9874  Test accuracy= 0.9874\n",
      "Epoch:  119   =====> Loss= 0.023650439  Validation accuracy= 0.9874  Test accuracy= 0.9879\n",
      "Epoch:  120   =====> Loss= 0.022322848  Validation accuracy= 0.9876  Test accuracy= 0.988\n",
      "Epoch:  121   =====> Loss= 0.023320520  Validation accuracy= 0.9888  Test accuracy= 0.9883\n",
      "Epoch:  122   =====> Loss= 0.021696907  Validation accuracy= 0.9876  Test accuracy= 0.9879\n",
      "Epoch:  123   =====> Loss= 0.022045037  Validation accuracy= 0.9884  Test accuracy= 0.9887\n",
      "Epoch:  124   =====> Loss= 0.021735647  Validation accuracy= 0.9878  Test accuracy= 0.9881\n",
      "Epoch:  125   =====> Loss= 0.021409377  Validation accuracy= 0.9874  Test accuracy= 0.9877\n",
      "Epoch:  126   =====> Loss= 0.021471361  Validation accuracy= 0.9884  Test accuracy= 0.9889\n",
      "Epoch:  127   =====> Loss= 0.020926045  Validation accuracy= 0.9872  Test accuracy= 0.9883\n",
      "Epoch:  128   =====> Loss= 0.020402798  Validation accuracy= 0.987  Test accuracy= 0.9881\n",
      "Epoch:  129   =====> Loss= 0.020623742  Validation accuracy= 0.9878  Test accuracy= 0.9886\n",
      "Epoch:  130   =====> Loss= 0.020756424  Validation accuracy= 0.9886  Test accuracy= 0.9886\n",
      "Epoch:  131   =====> Loss= 0.019686457  Validation accuracy= 0.9886  Test accuracy= 0.9888\n",
      "Epoch:  132   =====> Loss= 0.020024373  Validation accuracy= 0.988  Test accuracy= 0.9883\n",
      "Epoch:  133   =====> Loss= 0.020844101  Validation accuracy= 0.9872  Test accuracy= 0.9886\n",
      "Epoch:  134   =====> Loss= 0.018982376  Validation accuracy= 0.9888  Test accuracy= 0.9892\n",
      "Epoch:  135   =====> Loss= 0.019082260  Validation accuracy= 0.988  Test accuracy= 0.9879\n",
      "Epoch:  136   =====> Loss= 0.019065446  Validation accuracy= 0.9876  Test accuracy= 0.9891\n",
      "Epoch:  137   =====> Loss= 0.019546648  Validation accuracy= 0.9886  Test accuracy= 0.9892\n",
      "Epoch:  138   =====> Loss= 0.018960108  Validation accuracy= 0.9876  Test accuracy= 0.9892\n",
      "Epoch:  139   =====> Loss= 0.018055711  Validation accuracy= 0.9886  Test accuracy= 0.9886\n",
      "Epoch:  140   =====> Loss= 0.018643468  Validation accuracy= 0.988  Test accuracy= 0.9889\n",
      "Epoch:  141   =====> Loss= 0.018267117  Validation accuracy= 0.9882  Test accuracy= 0.9891\n",
      "Epoch:  142   =====> Loss= 0.018018327  Validation accuracy= 0.9882  Test accuracy= 0.9895\n",
      "Epoch:  143   =====> Loss= 0.018379094  Validation accuracy= 0.9876  Test accuracy= 0.9892\n",
      "Epoch:  144   =====> Loss= 0.017671612  Validation accuracy= 0.9874  Test accuracy= 0.989\n",
      "Epoch:  145   =====> Loss= 0.016998068  Validation accuracy= 0.9878  Test accuracy= 0.9884\n",
      "Epoch:  146   =====> Loss= 0.017596262  Validation accuracy= 0.9892  Test accuracy= 0.9892\n",
      "Epoch:  147   =====> Loss= 0.017063023  Validation accuracy= 0.9882  Test accuracy= 0.9884\n",
      "Epoch:  148   =====> Loss= 0.016875218  Validation accuracy= 0.9888  Test accuracy= 0.9902\n",
      "Epoch:  149   =====> Loss= 0.017785960  Validation accuracy= 0.9876  Test accuracy= 0.989\n",
      "Epoch:  150   =====> Loss= 0.016966669  Validation accuracy= 0.9878  Test accuracy= 0.9895\n",
      "Epoch:  151   =====> Loss= 0.016676137  Validation accuracy= 0.9888  Test accuracy= 0.9898\n",
      "Epoch:  152   =====> Loss= 0.015744936  Validation accuracy= 0.989  Test accuracy= 0.9894\n",
      "Epoch:  153   =====> Loss= 0.016398569  Validation accuracy= 0.9884  Test accuracy= 0.9895\n",
      "Epoch:  154   =====> Loss= 0.015774943  Validation accuracy= 0.9882  Test accuracy= 0.9892\n",
      "Epoch:  155   =====> Loss= 0.015730022  Validation accuracy= 0.9894  Test accuracy= 0.9892\n",
      "Epoch:  156   =====> Loss= 0.015811218  Validation accuracy= 0.9882  Test accuracy= 0.9889\n",
      "Epoch:  157   =====> Loss= 0.016122183  Validation accuracy= 0.9884  Test accuracy= 0.9884\n",
      "Epoch:  158   =====> Loss= 0.015315281  Validation accuracy= 0.9878  Test accuracy= 0.9886\n",
      "Epoch:  159   =====> Loss= 0.015055184  Validation accuracy= 0.9882  Test accuracy= 0.9895\n",
      "Epoch:  160   =====> Loss= 0.015479419  Validation accuracy= 0.988  Test accuracy= 0.9893\n",
      "Epoch:  161   =====> Loss= 0.014629433  Validation accuracy= 0.9888  Test accuracy= 0.9896\n",
      "Epoch:  162   =====> Loss= 0.014732498  Validation accuracy= 0.9884  Test accuracy= 0.9897\n",
      "Epoch:  163   =====> Loss= 0.014117643  Validation accuracy= 0.9886  Test accuracy= 0.9895\n",
      "Epoch:  164   =====> Loss= 0.014545544  Validation accuracy= 0.9884  Test accuracy= 0.9899\n",
      "Epoch:  165   =====> Loss= 0.014448897  Validation accuracy= 0.9886  Test accuracy= 0.9901\n",
      "Epoch:  166   =====> Loss= 0.014977406  Validation accuracy= 0.9884  Test accuracy= 0.9895\n",
      "Epoch:  167   =====> Loss= 0.014063026  Validation accuracy= 0.989  Test accuracy= 0.9898\n",
      "Epoch:  168   =====> Loss= 0.013537712  Validation accuracy= 0.988  Test accuracy= 0.99\n",
      "Epoch:  169   =====> Loss= 0.014221393  Validation accuracy= 0.988  Test accuracy= 0.9893\n",
      "Epoch:  170   =====> Loss= 0.014508333  Validation accuracy= 0.988  Test accuracy= 0.9886\n",
      "Epoch:  171   =====> Loss= 0.012804839  Validation accuracy= 0.989  Test accuracy= 0.9894\n",
      "Epoch:  172   =====> Loss= 0.013299452  Validation accuracy= 0.9882  Test accuracy= 0.9892\n",
      "Epoch:  173   =====> Loss= 0.013026615  Validation accuracy= 0.9898  Test accuracy= 0.9895\n",
      "Epoch:  174   =====> Loss= 0.013447625  Validation accuracy= 0.988  Test accuracy= 0.9898\n",
      "Epoch:  175   =====> Loss= 0.012404188  Validation accuracy= 0.9882  Test accuracy= 0.9895\n",
      "Epoch:  176   =====> Loss= 0.013155411  Validation accuracy= 0.988  Test accuracy= 0.9899\n",
      "Epoch:  177   =====> Loss= 0.013614445  Validation accuracy= 0.988  Test accuracy= 0.9901\n",
      "Epoch:  178   =====> Loss= 0.012569072  Validation accuracy= 0.9888  Test accuracy= 0.9896\n",
      "Epoch:  179   =====> Loss= 0.012858638  Validation accuracy= 0.9884  Test accuracy= 0.9897\n",
      "Epoch:  180   =====> Loss= 0.012033841  Validation accuracy= 0.9886  Test accuracy= 0.9895\n",
      "Epoch:  181   =====> Loss= 0.012924304  Validation accuracy= 0.9882  Test accuracy= 0.9891\n",
      "Epoch:  182   =====> Loss= 0.011285193  Validation accuracy= 0.989  Test accuracy= 0.9904\n",
      "Epoch:  183   =====> Loss= 0.012636733  Validation accuracy= 0.988  Test accuracy= 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  184   =====> Loss= 0.011654639  Validation accuracy= 0.9886  Test accuracy= 0.9898\n",
      "Epoch:  185   =====> Loss= 0.011734363  Validation accuracy= 0.987  Test accuracy= 0.9902\n",
      "Epoch:  186   =====> Loss= 0.012098399  Validation accuracy= 0.9884  Test accuracy= 0.9896\n",
      "Epoch:  187   =====> Loss= 0.011165604  Validation accuracy= 0.9884  Test accuracy= 0.9889\n",
      "Epoch:  188   =====> Loss= 0.012288753  Validation accuracy= 0.989  Test accuracy= 0.9901\n",
      "Epoch:  189   =====> Loss= 0.010551964  Validation accuracy= 0.9886  Test accuracy= 0.99\n",
      "Epoch:  190   =====> Loss= 0.012254596  Validation accuracy= 0.9882  Test accuracy= 0.9893\n",
      "Epoch:  191   =====> Loss= 0.010705987  Validation accuracy= 0.9884  Test accuracy= 0.9905\n",
      "Epoch:  192   =====> Loss= 0.010890607  Validation accuracy= 0.9892  Test accuracy= 0.9898\n",
      "Epoch:  193   =====> Loss= 0.011106896  Validation accuracy= 0.988  Test accuracy= 0.9899\n",
      "Epoch:  194   =====> Loss= 0.010891080  Validation accuracy= 0.989  Test accuracy= 0.9906\n",
      "Epoch:  195   =====> Loss= 0.010817055  Validation accuracy= 0.9888  Test accuracy= 0.9895\n",
      "Epoch:  196   =====> Loss= 0.011130803  Validation accuracy= 0.9898  Test accuracy= 0.9894\n",
      "Epoch:  197   =====> Loss= 0.009879846  Validation accuracy= 0.9884  Test accuracy= 0.99\n",
      "Epoch:  198   =====> Loss= 0.010734334  Validation accuracy= 0.9886  Test accuracy= 0.9904\n",
      "Epoch:  199   =====> Loss= 0.010990067  Validation accuracy= 0.9884  Test accuracy= 0.9896\n",
      "Epoch:  200   =====> Loss= 0.009702761  Validation accuracy= 0.989  Test accuracy= 0.9901\n",
      "Training Finished!\n",
      "Test accuracy: 0.9901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.1126,\n",
       "  0.1126,\n",
       "  0.098999999,\n",
       "  0.0986,\n",
       "  0.1126,\n",
       "  0.086800002,\n",
       "  0.1582,\n",
       "  0.2568,\n",
       "  0.55159998,\n",
       "  0.75139999,\n",
       "  0.8326,\n",
       "  0.8642,\n",
       "  0.89480001,\n",
       "  0.91780001,\n",
       "  0.92940003,\n",
       "  0.94019997,\n",
       "  0.94639999,\n",
       "  0.94880003,\n",
       "  0.95480001,\n",
       "  0.95880002,\n",
       "  0.9612,\n",
       "  0.96460003,\n",
       "  0.96640003,\n",
       "  0.96759999,\n",
       "  0.96960002,\n",
       "  0.97000003,\n",
       "  0.97039998,\n",
       "  0.97100002,\n",
       "  0.97299999,\n",
       "  0.97399998,\n",
       "  0.97259998,\n",
       "  0.9738,\n",
       "  0.97600001,\n",
       "  0.97719997,\n",
       "  0.97539997,\n",
       "  0.97719997,\n",
       "  0.97659999,\n",
       "  0.97640002,\n",
       "  0.97600001,\n",
       "  0.97839999,\n",
       "  0.9788,\n",
       "  0.97799999,\n",
       "  0.97759998,\n",
       "  0.9788,\n",
       "  0.97979999,\n",
       "  0.98000002,\n",
       "  0.9806,\n",
       "  0.98220003,\n",
       "  0.98040003,\n",
       "  0.98000002,\n",
       "  0.97979999,\n",
       "  0.98180002,\n",
       "  0.98079997,\n",
       "  0.98159999,\n",
       "  0.9824,\n",
       "  0.98199999,\n",
       "  0.98280001,\n",
       "  0.98299998,\n",
       "  0.98339999,\n",
       "  0.98460001,\n",
       "  0.98400003,\n",
       "  0.98360002,\n",
       "  0.98339999,\n",
       "  0.98360002,\n",
       "  0.98360002,\n",
       "  0.9842,\n",
       "  0.98199999,\n",
       "  0.98439997,\n",
       "  0.98360002,\n",
       "  0.98460001,\n",
       "  0.98540002,\n",
       "  0.98379999,\n",
       "  0.98500001,\n",
       "  0.98460001,\n",
       "  0.98479998,\n",
       "  0.98460001,\n",
       "  0.98640001,\n",
       "  0.98540002,\n",
       "  0.98640001,\n",
       "  0.98519999,\n",
       "  0.98680001,\n",
       "  0.98619998,\n",
       "  0.98540002,\n",
       "  0.9874,\n",
       "  0.98640001,\n",
       "  0.9874,\n",
       "  0.98500001,\n",
       "  0.98640001,\n",
       "  0.9878,\n",
       "  0.98619998,\n",
       "  0.98680001,\n",
       "  0.98720002,\n",
       "  0.9878,\n",
       "  0.98720002,\n",
       "  0.98699999,\n",
       "  0.98799998,\n",
       "  0.98820001,\n",
       "  0.9878,\n",
       "  0.98720002,\n",
       "  0.9878,\n",
       "  0.98760003,\n",
       "  0.98799998,\n",
       "  0.98839998,\n",
       "  0.98839998,\n",
       "  0.98659998,\n",
       "  0.9874,\n",
       "  0.98699999,\n",
       "  0.98720002,\n",
       "  0.9874,\n",
       "  0.98799998,\n",
       "  0.98680001,\n",
       "  0.98799998,\n",
       "  0.9878,\n",
       "  0.9878,\n",
       "  0.98699999,\n",
       "  0.98799998,\n",
       "  0.9878,\n",
       "  0.9874,\n",
       "  0.9874,\n",
       "  0.98760003,\n",
       "  0.98879999,\n",
       "  0.98760003,\n",
       "  0.98839998,\n",
       "  0.9878,\n",
       "  0.9874,\n",
       "  0.98839998,\n",
       "  0.98720002,\n",
       "  0.98699999,\n",
       "  0.9878,\n",
       "  0.98860002,\n",
       "  0.98860002,\n",
       "  0.98799998,\n",
       "  0.98720002,\n",
       "  0.98879999,\n",
       "  0.98799998,\n",
       "  0.98760003,\n",
       "  0.98860002,\n",
       "  0.98760003,\n",
       "  0.98860002,\n",
       "  0.98799998,\n",
       "  0.98820001,\n",
       "  0.98820001,\n",
       "  0.98760003,\n",
       "  0.9874,\n",
       "  0.9878,\n",
       "  0.9892,\n",
       "  0.98820001,\n",
       "  0.98879999,\n",
       "  0.98760003,\n",
       "  0.9878,\n",
       "  0.98879999,\n",
       "  0.98900002,\n",
       "  0.98839998,\n",
       "  0.98820001,\n",
       "  0.98940003,\n",
       "  0.98820001,\n",
       "  0.98839998,\n",
       "  0.9878,\n",
       "  0.98820001,\n",
       "  0.98799998,\n",
       "  0.98879999,\n",
       "  0.98839998,\n",
       "  0.98860002,\n",
       "  0.98839998,\n",
       "  0.98860002,\n",
       "  0.98839998,\n",
       "  0.98900002,\n",
       "  0.98799998,\n",
       "  0.98799998,\n",
       "  0.98799998,\n",
       "  0.98900002,\n",
       "  0.98820001,\n",
       "  0.98979998,\n",
       "  0.98799998,\n",
       "  0.98820001,\n",
       "  0.98799998,\n",
       "  0.98799998,\n",
       "  0.98879999,\n",
       "  0.98839998,\n",
       "  0.98860002,\n",
       "  0.98820001,\n",
       "  0.98900002,\n",
       "  0.98799998,\n",
       "  0.98860002,\n",
       "  0.98699999,\n",
       "  0.98839998,\n",
       "  0.98839998,\n",
       "  0.98900002,\n",
       "  0.98860002,\n",
       "  0.98820001,\n",
       "  0.98839998,\n",
       "  0.9892,\n",
       "  0.98799998,\n",
       "  0.98900002,\n",
       "  0.98879999,\n",
       "  0.98979998,\n",
       "  0.98839998,\n",
       "  0.98860002,\n",
       "  0.98839998,\n",
       "  0.98900002],\n",
       " [0.1135,\n",
       "  0.1135,\n",
       "  0.1009,\n",
       "  0.101,\n",
       "  0.1135,\n",
       "  0.089199997,\n",
       "  0.1736,\n",
       "  0.2669,\n",
       "  0.55540001,\n",
       "  0.7543,\n",
       "  0.83450001,\n",
       "  0.86970001,\n",
       "  0.89109999,\n",
       "  0.91329998,\n",
       "  0.92540002,\n",
       "  0.93589997,\n",
       "  0.93919998,\n",
       "  0.94739997,\n",
       "  0.95340002,\n",
       "  0.95709997,\n",
       "  0.95880002,\n",
       "  0.96319997,\n",
       "  0.96539998,\n",
       "  0.96700001,\n",
       "  0.96929997,\n",
       "  0.97070003,\n",
       "  0.9716,\n",
       "  0.972,\n",
       "  0.97390002,\n",
       "  0.97439998,\n",
       "  0.97409999,\n",
       "  0.97490001,\n",
       "  0.97689998,\n",
       "  0.97839999,\n",
       "  0.97909999,\n",
       "  0.97619998,\n",
       "  0.97869998,\n",
       "  0.98040003,\n",
       "  0.9806,\n",
       "  0.98070002,\n",
       "  0.98079997,\n",
       "  0.9817,\n",
       "  0.9817,\n",
       "  0.98180002,\n",
       "  0.98269999,\n",
       "  0.98150003,\n",
       "  0.98140001,\n",
       "  0.98269999,\n",
       "  0.98339999,\n",
       "  0.98119998,\n",
       "  0.98220003,\n",
       "  0.98329997,\n",
       "  0.98339999,\n",
       "  0.98379999,\n",
       "  0.98390001,\n",
       "  0.98430002,\n",
       "  0.98460001,\n",
       "  0.98390001,\n",
       "  0.98500001,\n",
       "  0.9849,\n",
       "  0.98619998,\n",
       "  0.98519999,\n",
       "  0.9853,\n",
       "  0.98509997,\n",
       "  0.98379999,\n",
       "  0.98500001,\n",
       "  0.98449999,\n",
       "  0.98619998,\n",
       "  0.98559999,\n",
       "  0.98540002,\n",
       "  0.98580003,\n",
       "  0.98479998,\n",
       "  0.98559999,\n",
       "  0.98519999,\n",
       "  0.98479998,\n",
       "  0.98559999,\n",
       "  0.98699999,\n",
       "  0.9867,\n",
       "  0.986,\n",
       "  0.98640001,\n",
       "  0.9867,\n",
       "  0.98650002,\n",
       "  0.98570001,\n",
       "  0.98680001,\n",
       "  0.98589998,\n",
       "  0.9878,\n",
       "  0.98640001,\n",
       "  0.98629999,\n",
       "  0.9878,\n",
       "  0.98589998,\n",
       "  0.98720002,\n",
       "  0.98629999,\n",
       "  0.98710001,\n",
       "  0.98769999,\n",
       "  0.98710001,\n",
       "  0.9874,\n",
       "  0.98769999,\n",
       "  0.98790002,\n",
       "  0.98680001,\n",
       "  0.98799998,\n",
       "  0.9867,\n",
       "  0.9878,\n",
       "  0.98839998,\n",
       "  0.98799998,\n",
       "  0.9867,\n",
       "  0.98729998,\n",
       "  0.98680001,\n",
       "  0.98769999,\n",
       "  0.9878,\n",
       "  0.98750001,\n",
       "  0.9874,\n",
       "  0.9878,\n",
       "  0.98830003,\n",
       "  0.9885,\n",
       "  0.98799998,\n",
       "  0.98750001,\n",
       "  0.98799998,\n",
       "  0.9874,\n",
       "  0.98790002,\n",
       "  0.98799998,\n",
       "  0.98830003,\n",
       "  0.98790002,\n",
       "  0.98869997,\n",
       "  0.98809999,\n",
       "  0.98769999,\n",
       "  0.98890001,\n",
       "  0.98830003,\n",
       "  0.98809999,\n",
       "  0.98860002,\n",
       "  0.98860002,\n",
       "  0.98879999,\n",
       "  0.98830003,\n",
       "  0.98860002,\n",
       "  0.9892,\n",
       "  0.98790002,\n",
       "  0.98909998,\n",
       "  0.9892,\n",
       "  0.9892,\n",
       "  0.98860002,\n",
       "  0.98890001,\n",
       "  0.98909998,\n",
       "  0.98949999,\n",
       "  0.9892,\n",
       "  0.98900002,\n",
       "  0.98839998,\n",
       "  0.9892,\n",
       "  0.98839998,\n",
       "  0.99019998,\n",
       "  0.98900002,\n",
       "  0.98949999,\n",
       "  0.98979998,\n",
       "  0.98940003,\n",
       "  0.98949999,\n",
       "  0.9892,\n",
       "  0.9892,\n",
       "  0.98890001,\n",
       "  0.98839998,\n",
       "  0.98860002,\n",
       "  0.98949999,\n",
       "  0.98930001,\n",
       "  0.9896,\n",
       "  0.98970002,\n",
       "  0.98949999,\n",
       "  0.98989999,\n",
       "  0.99010003,\n",
       "  0.98949999,\n",
       "  0.98979998,\n",
       "  0.99000001,\n",
       "  0.98930001,\n",
       "  0.98860002,\n",
       "  0.98940003,\n",
       "  0.9892,\n",
       "  0.98949999,\n",
       "  0.98979998,\n",
       "  0.98949999,\n",
       "  0.98989999,\n",
       "  0.99010003,\n",
       "  0.9896,\n",
       "  0.98970002,\n",
       "  0.98949999,\n",
       "  0.98909998,\n",
       "  0.99040002,\n",
       "  0.99000001,\n",
       "  0.98979998,\n",
       "  0.99019998,\n",
       "  0.9896,\n",
       "  0.98890001,\n",
       "  0.99010003,\n",
       "  0.99000001,\n",
       "  0.98930001,\n",
       "  0.99049997,\n",
       "  0.98979998,\n",
       "  0.98989999,\n",
       "  0.99059999,\n",
       "  0.98949999,\n",
       "  0.98940003,\n",
       "  0.99000001,\n",
       "  0.99040002,\n",
       "  0.9896,\n",
       "  0.99010003])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(learning_rate, training_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.6 </b> : Use tensorBoard to visualise and save the LeNet5 Graph and all learning curves. \n",
    "Save all obtained figures in the folder **\"TP2/MNIST_99_Challenge_Figures\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Graph Model</h1>\n",
    "<img src=\"MNIST_99_Challenge_Figures/graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2 </b> : LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> Question 2.2.1 </b>  Change the sigmoid function with a ReLU :\n",
    "\n",
    "- Retrain your network with SGD and AdamOptimizer and then fill the table above  :\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent         |AdamOptimizer |\n",
    "| -------------        |: -------------: | ---------:   \n",
    "| Validation Accuracy  |         |    |      \n",
    "| Testing Accuracy     |           |    |       \n",
    "| Training Time        |           |        |  |  \n",
    "\n",
    "\n",
    "- Try with different learning rates for each Optimizer (0.0001 and 0.001 ) and different Batch sizes (50 and 128) for 20000 Epochs. \n",
    "\n",
    "- For each optimizer, plot (on the same curve) the **testing accuracies** function to **(learning rate, batch size)** \n",
    "\n",
    "\n",
    "\n",
    "- Did you reach the 99% accuracy ? What are the optimal parametres that gave you the best results? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001 Batch size: 50 optimizer: SGD\n",
      "\t====> Time: 6176.0356233119965 Validation accuracy: 0.9858 Test accuracy: 0.9842 \n",
      "----------------------------------------------\n",
      "Learning rate: 0.001 Batch size: 128 optimizer: SGD\n",
      "\t====> Time: 5185.878306150436 Validation accuracy: 0.9814 Test accuracy: 0.9822 \n",
      "----------------------------------------------\n",
      "Learning rate: 0.0001 Batch size: 50 optimizer: SGD\n",
      "\t====> Time: 6140.959002494812 Validation accuracy: 0.9626 Test accuracy: 0.9602 \n",
      "----------------------------------------------\n",
      "Learning rate: 0.0001 Batch size: 128 optimizer: SGD\n",
      "\t====> Time: 5140.468836069107 Validation accuracy: 0.9352 Test accuracy: 0.9329 \n",
      "----------------------------------------------\n",
      "Learning rate: 0.001 Batch size: 50 optimizer: Adam\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.0001]\n",
    "batch_sizes = [50, 128]\n",
    "optNames = [\"SGD\", \"Adam\"]\n",
    "training_epochs = 200\n",
    "disp_step = 20\n",
    "# training_epochs = 3\n",
    "# disp_step = 1\n",
    "\n",
    "results = []\n",
    "for on in optNames:\n",
    "    for lr in learning_rates:\n",
    "        for bs in batch_sizes:\n",
    "            print(\"Learning rate:\", lr, \"Batch size:\", bs, \"optimizer:\", on)\n",
    "            t1 = time.time()\n",
    "            val_history, test_history = train(learning_rate=lr, \\\n",
    "                                            training_epochs=training_epochs, batch_size=bs, \\\n",
    "                                            display_step=disp_step, optFunction=on, verbose=False, transfer=\"ReLU\")\n",
    "            t2 = time.time() - t1\n",
    "            print(\"\\t====> Time:\", t2, \"Validation accuracy:\", val_history[-1], \"Test accuracy:\", test_history[-1], \\\n",
    "                  \"\\n----------------------------------------------\")\n",
    "            results.append((lr, bs, on, t2, test_history, val_history))\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(211)\n",
    "max_epochs = max([len(results[0][4]), len(results[1][4]), len(results[2][4]), len(results[3][4])])\n",
    "plt.plot(np.arange(len(results[0][4])), results[0][4], c=\"r\", \\\n",
    "         label=\"Learning rate:\" + str(results[0][0]) + \" Batch size:\" + str(results[0][1]))\n",
    "plt.plot(np.arange(len(results[1][4])), results[1][4], c=\"b\", \\\n",
    "         label=\"Learning rate:\" + str(results[1][0]) + \" Batch size:\" + str(results[1][1]))\n",
    "plt.plot(np.arange(len(results[2][4])), results[2][4], c=\"g\", \\\n",
    "         label=\"Learning rate:\" + str(results[2][0]) + \" Batch size:\" + str(results[2][1]))\n",
    "plt.plot(np.arange(len(results[3][4])), results[3][4], c=\"k\", \\\n",
    "         label=\"Learning rate:\" + str(results[3][0]) + \" Batch size:\" + str(results[3][1]))\n",
    "plt.legend()\n",
    "plt.title(\"Test accuracy over epochs for SGD\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlim((0, max_epochs))\n",
    "\n",
    "plt.subplot(212)\n",
    "max_epochs = max([len(results[4][4]), len(results[5][4]), len(results[6][4]), len(results[7][4])])\n",
    "plt.plot(np.arange(len(results[4][4])), results[4][4], c=\"r\", \\\n",
    "         label=\"Learning rate:\" + str(results[4][0]) + \" Batch size:\" + str(results[4][1]))\n",
    "plt.plot(np.arange(len(results[5][4])), results[5][4], c=\"b\", \\\n",
    "         label=\"Learning rate:\" + str(results[5][0]) + \" Batch size:\" + str(results[5][1]))\n",
    "plt.plot(np.arange(len(results[6][4])), results[6][4], c=\"g\", \\\n",
    "         label=\"Learning rate:\" + str(results[6][0]) + \" Batch size:\" + str(results[6][1]))\n",
    "plt.plot(np.arange(len(results[7][4])), results[7][4], c=\"k\", \\\n",
    "         label=\"Learning rate:\" + str(results[7][0]) + \" Batch size:\" + str(results[7][1]))\n",
    "plt.legend()\n",
    "plt.title(\"Test accuracy over epochs for Adam Optimizer\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlim((0, max_epochs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.2 </b>  What about applying a dropout layer on the Fully connected layer and then retraining the model with the best Optimizer and parameters (Learning rate and Batch size) obtained in *Question 2.2.1*  ? (probability to keep units=0.75). For this stage ensure that the keep prob is set to 1.0 to evaluate the \n",
    "performance of the network including all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "bs = 50\n",
    "opt = \"Adam\"\n",
    "training_epochs = 200\n",
    "disp_step = 20\n",
    "kp = 0.75\n",
    "\n",
    "t1 = time.time()\n",
    "val_history_do, test_history_do = train(learning_rate=lr, training_epochs=training_epochs, batch_size=bs, \\\n",
    "                    display_step=disp_step, optFunction=opt, verbose=False, transfer=\"ReLU\", keep_probability=kp)\n",
    "t2 = time.time() - t1\n",
    "print(\"====> Time:\", t2, \"Validation accuracy:\", val_history_do[-1], \"Test accuracy:\", test_history_do[-1])\n",
    "print(\"Optimization Finished!\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
