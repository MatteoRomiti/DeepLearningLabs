{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 2 - 3 Hours </h1>\n",
    "<h1 style=\"text-align:center\"> Convolutional Neural Network (CNN) for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b> Student 1:</b> Daniele Reda  \n",
    "<br>\n",
    "<b> Student 2:</b> Matteo Romiti\n",
    "</div> \n",
    " \n",
    "The aim of this session is to practice with Convolutional Neural Networks. Answers and experiments should be made by groups of one or two students. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "\n",
    "Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an pdf document using print as PDF (Ctrl+P). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed by May 29th 2017.\n",
    "\n",
    "Send you pdf file to benoit.huet@eurecom.fr and olfa.ben-ahmed@eurecom.fr using **[DeepLearning_lab2]** as Subject of your email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last Lab Session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%.  Can  you do better than these results using a deep CNN ?\n",
    "In this Lab Session, you will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks:  **LeNet-5** to go to  more than 99% of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data in TensorFlow\n",
    "Run the cell above to load the MNIST data that comes  with TensorFlow. You will use this data in **Section 1** and **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Image Shape:    (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "print(\"Image Shape:    {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : My First Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow the example :\n",
    "**y=softmax(Wx+b)** seen in the DeepLearing course last week. \n",
    "\n",
    "This model reaches an accuracy of about 92 %.\n",
    "You will also learn how to launch the tensorBoard https://www.tensorflow.org/get_started/summaries_and_tensorboard to  visualize the computation graph, statistics and learning curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : Read carefully the code in the cell below. Run it to perform training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  02   =====> Loss= 0.732524195\n",
      "Epoch:  04   =====> Loss= 0.536677648\n",
      "Epoch:  06   =====> Loss= 0.470880614\n",
      "Epoch:  08   =====> Loss= 0.435697370\n",
      "Epoch:  10   =====> Loss= 0.414145648\n",
      "Epoch:  12   =====> Loss= 0.396247747\n",
      "Epoch:  14   =====> Loss= 0.384974787\n",
      "Epoch:  16   =====> Loss= 0.375682483\n",
      "Epoch:  18   =====> Loss= 0.367070107\n",
      "Epoch:  20   =====> Loss= 0.360158942\n",
      "Epoch:  22   =====> Loss= 0.352310474\n",
      "Epoch:  24   =====> Loss= 0.348237286\n",
      "Epoch:  26   =====> Loss= 0.345345280\n",
      "Epoch:  28   =====> Loss= 0.342196290\n",
      "Epoch:  30   =====> Loss= 0.336128746\n",
      "Epoch:  32   =====> Loss= 0.332090786\n",
      "Epoch:  34   =====> Loss= 0.329738538\n",
      "Epoch:  36   =====> Loss= 0.326926193\n",
      "Epoch:  38   =====> Loss= 0.323137202\n",
      "Epoch:  40   =====> Loss= 0.324002850\n",
      "Epoch:  42   =====> Loss= 0.319857761\n",
      "Epoch:  44   =====> Loss= 0.317417873\n",
      "Epoch:  46   =====> Loss= 0.317157803\n",
      "Epoch:  48   =====> Loss= 0.313185978\n",
      "Epoch:  50   =====> Loss= 0.314381154\n",
      "Epoch:  52   =====> Loss= 0.312158206\n",
      "Epoch:  54   =====> Loss= 0.309910980\n",
      "Epoch:  56   =====> Loss= 0.309928803\n",
      "Epoch:  58   =====> Loss= 0.306476584\n",
      "Epoch:  60   =====> Loss= 0.305252086\n",
      "Epoch:  62   =====> Loss= 0.306394995\n",
      "Epoch:  64   =====> Loss= 0.302055581\n",
      "Epoch:  66   =====> Loss= 0.304512259\n",
      "Epoch:  68   =====> Loss= 0.300430740\n",
      "Epoch:  70   =====> Loss= 0.299678170\n",
      "Epoch:  72   =====> Loss= 0.302024589\n",
      "Epoch:  74   =====> Loss= 0.297574734\n",
      "Epoch:  76   =====> Loss= 0.299216203\n",
      "Epoch:  78   =====> Loss= 0.297055104\n",
      "Epoch:  80   =====> Loss= 0.297850100\n",
      "Epoch:  82   =====> Loss= 0.293244862\n",
      "Epoch:  84   =====> Loss= 0.296930625\n",
      "Epoch:  86   =====> Loss= 0.295676767\n",
      "Epoch:  88   =====> Loss= 0.296824119\n",
      "Epoch:  90   =====> Loss= 0.289912013\n",
      "Epoch:  92   =====> Loss= 0.292734193\n",
      "Epoch:  94   =====> Loss= 0.292083733\n",
      "Epoch:  96   =====> Loss= 0.290795056\n",
      "Epoch:  98   =====> Loss= 0.288702441\n",
      "Epoch:  100   =====> Loss= 0.288176962\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9206\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "# import tensorflow as tf\n",
    "\n",
    "\n",
    "#STEP 1\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 128\n",
    "display_step = 2\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "#STEP 2 \n",
    "\n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2  </b>: Using Tensorboard, we can  now visualize the created graph, giving you an overview of your architecture and how all of the major components  are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch tensorBoard: \n",
    "- Go to the **TP2** folder, \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir= log_files/\"**, it will generate an http link ,ex http://666.6.6.6:6006,\n",
    "- Copy this  link into your web browser \n",
    "\n",
    "\n",
    "Enjoy It !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : The 99% MNIST Challenge !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : LeNet5 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are familar with **tensorFlow** and **tensorBoard**, you are in this section to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "In more advanced step you will make some optimizations to get more than 99% of accuracy. The best model can get to over 99.7% accuracy! \n",
    "\n",
    "For more information, have a look at this list of results : http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"lenet.png\",width=\"800\" height=\"600\" align=\"center\">\n",
    "<center><span>Figure 1: Lenet 5 </span></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1: Convolutional.** The output shape should be 28x28x6 **Activation.** sigmoid **Pooling.** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 10x10x16. **Activation.** sigmoid **Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use **flatten*  from tensorflow.contrib.layers import flatten\n",
    "\n",
    "**Layer 3: Fully Connected.** This should have 120 outputs. **Activation.** sigmoid\n",
    "\n",
    "**Layer 4: Fully Connected.** This should have 84 outputs. **Activation.** sigmoid\n",
    "\n",
    "**Layer 5: Fully Connected.** This should have 10 outputs **Activation.** softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.1 </b>  Implement the Neural Network architecture described above.\n",
    "For that, your will use classes and functions from  https://www.tensorflow.org/api_docs/python/tf/nn. \n",
    "\n",
    "We give you some helper functions for weigths and bias initilization. Also you can refer to section 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper functions  for weigths and bias initilization \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride, padding_):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LeNet5_Model(data):    \n",
    "    # your implementation goes here\n",
    "    \n",
    "    #first convolutional layer\n",
    "    W_conv1 = weight_variable([5, 5, 1, 6]) ## [filter_width, filter_height, depth_image_in, depth_image_out]\n",
    "    b_conv1 = bias_variable([6])\n",
    "    h_conv1 = tf.sigmoid(conv2d(data, W_conv1, 1, 'SAME') + b_conv1)\n",
    "    pool1 = tf.nn.pool(h_conv1, [2,2], \"MAX\", 'VALID', strides=[2,2])\n",
    "    \n",
    "    #second convolutional layer\n",
    "    W_conv2 = weight_variable([5, 5, 6, 16])\n",
    "    b_conv2 = bias_variable([16])\n",
    "    h_conv2 = tf.sigmoid(conv2d(pool1, W_conv2, 1, 'VALID') + b_conv2)\n",
    "    pool2 = tf.nn.pool(h_conv2, [2,2], \"MAX\", 'VALID', strides=[2,2])\n",
    "    \n",
    "    #first fully connected layer\n",
    "    s = pool2.get_shape().as_list()\n",
    "    flattened_length = s[1] * s[2] * s[3]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, flattened_length])\n",
    "    W_fc1 = weight_variable([1*5*5*16, 120])\n",
    "    b_fc1 = bias_variable([120])\n",
    "    h_fc1 = tf.sigmoid(tf.matmul(pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    #second fully connected layer\n",
    "    W_fc2 = weight_variable([120, 84])\n",
    "    b_fc2 = bias_variable([84])\n",
    "    h_fc2 = tf.sigmoid(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "    \n",
    "    #third fully connected layer\n",
    "    W_fc3 = weight_variable([84, 10])\n",
    "    b_fc3 = bias_variable([10])\n",
    "    h_fc3 = tf.nn.softmax(tf.matmul(h_fc2, W_fc3) + b_fc3)\n",
    "    \n",
    "    return h_fc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.2. </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59706\n"
     ]
    }
   ],
   "source": [
    "# first conv\n",
    "pconv1 = 5*5*1*6 # filter_height * filter_width * channels_in * num_feature_maps\n",
    "# second conv\n",
    "pconv2 = 5*5*1*16 # filter_height * filter_width * channels_in * num_feature_maps\n",
    "# first fcl\n",
    "pfcl1 = 5*5*16*120 # fcl_input_size * fcl_output_size\n",
    "pfcl1# second fcl\n",
    "pfcl2 = 84*120 # fcl_input_size * fcl_output_size\n",
    "# third fcl\n",
    "pfcl3 = 84*10 # fcl_input_size * fcl_output_size\n",
    "pbias = 6+16+120+84+10 # all the biases\n",
    "total = pbias + pfcl1 + pfcl2 + pfcl3 + pconv2 + pconv1\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.3. </b>  Start the training with the parameters cited below:\n",
    "\n",
    "     Learning rate : 0.1\n",
    "     Loss Function : Cross entropy\n",
    "     Optimizer: SGD\n",
    "     Number of training iterations : 10000\n",
    "     Batch size : 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "training_epochs = 200\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, y):\n",
    "    accuracy = acc.eval({x: mnist.test.images, y: mnist.test.labels})\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.5. </b>  Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(learning_rate, training_epochs, batch_size, display_step = 1, \\\n",
    "          logs_path='log_files/', optFunction=\"SGC\", verbose=True):\n",
    "    \n",
    "    optFunctions = {\"SGC\":tf.train.GradientDescentOptimizer, \"ReLU\":tf.train.AdamOptimizer}\n",
    "    \n",
    "    # Erase previous graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 28, 28, 1], name='InputData')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "    # Construct model\n",
    "    with tf.name_scope('Model'):\n",
    "        pred = LeNet5_Model(x)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    with tf.name_scope('Loss'):\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "        #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "    with tf.name_scope(optFunction):\n",
    "        optimizer = optFunctions[optFunction](learning_rate).minimize(cost)\n",
    "\n",
    "    # Evaluate model\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a summary to monitor cost tensor\n",
    "    tf.summary.scalar(\"Loss\", cost)\n",
    "    # Create a summary to monitor accuracy tensor\n",
    "    tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    x_val, y_val = mnist.validation.images.reshape(-1, 28, 28, 1), mnist.validation.labels\n",
    "    x_test, y_test = mnist.test.images.reshape(-1, 28, 28, 1), mnist.test.labels\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        if verbose is True:\n",
    "            print(\"Start Training!\")\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        saver = tf.train.Saver()\n",
    "        #Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            #Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "                batch_xs = batch_xs.reshape(-1, 28, 28, 1)\n",
    "                # Run optimization op (backprop), cost op (to get loss value)\n",
    "                # and summary nodes\n",
    "                _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                         feed_dict={x: batch_xs, y: batch_ys})\n",
    "                 # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "            # Display logs per epoch step\n",
    "            acc = accuracy.eval({x: x_val, y: y_val})\n",
    "            saver.save(sess, 'Models/model_' + str(learning_rate) + '_' + str(batch_size) + '_' + optFunction)\n",
    "            if verbose is True and (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \\\n",
    "                      \"  =====> Loss=\", \"{:.9f}\".format(avg_cost), \" Training accuracy=\", acc)\n",
    "            if acc>=0.99:\n",
    "                if verbose is True:\n",
    "                    print(\"Accuracy over 99%% reached after %d epochs\" %(epoch+1))\n",
    "                break\n",
    "                \n",
    "        if verbose is True:\n",
    "            print(\"Training Finished!\")\n",
    "            # Test model\n",
    "            # Calculate accuracy\n",
    "            print(\"Test accuracy:\", accuracy.eval({x: x_test, y: y_test}))\n",
    "        \n",
    "        val_acc = accuracy.eval({x: x_val, y: y_val})\n",
    "        test_acc = accuracy.eval({x: x_test, y: y_test})\n",
    "    return val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "Epoch:  01   =====> Loss= 2.306923388  Training accuracy= 0.1126\n",
      "Epoch:  02   =====> Loss= 2.305711344  Training accuracy= 0.0986\n",
      "Epoch:  03   =====> Loss= 2.305181658  Training accuracy= 0.0986\n",
      "Epoch:  04   =====> Loss= 2.304064671  Training accuracy= 0.0986\n",
      "Epoch:  05   =====> Loss= 2.302289627  Training accuracy= 0.112\n",
      "Epoch:  06   =====> Loss= 2.298982566  Training accuracy= 0.11\n",
      "Epoch:  07   =====> Loss= 2.288852153  Training accuracy= 0.1904\n",
      "Epoch:  08   =====> Loss= 2.172151118  Training accuracy= 0.3124\n",
      "Epoch:  09   =====> Loss= 1.590616186  Training accuracy= 0.6548\n",
      "Epoch:  10   =====> Loss= 0.923477484  Training accuracy= 0.7828\n",
      "Epoch:  11   =====> Loss= 0.641152425  Training accuracy= 0.8538\n",
      "Epoch:  12   =====> Loss= 0.476603856  Training accuracy= 0.8866\n",
      "Epoch:  13   =====> Loss= 0.369493994  Training accuracy= 0.913\n",
      "Epoch:  14   =====> Loss= 0.301952966  Training accuracy= 0.9292\n",
      "Epoch:  15   =====> Loss= 0.256582659  Training accuracy= 0.9388\n",
      "Epoch:  16   =====> Loss= 0.222512772  Training accuracy= 0.947\n",
      "Epoch:  17   =====> Loss= 0.197709738  Training accuracy= 0.9508\n",
      "Epoch:  18   =====> Loss= 0.176510492  Training accuracy= 0.9542\n",
      "Epoch:  19   =====> Loss= 0.166109884  Training accuracy= 0.9606\n",
      "Epoch:  20   =====> Loss= 0.150210114  Training accuracy= 0.965\n",
      "Epoch:  21   =====> Loss= 0.138426067  Training accuracy= 0.9678\n",
      "Epoch:  22   =====> Loss= 0.129115178  Training accuracy= 0.9678\n",
      "Epoch:  23   =====> Loss= 0.123906451  Training accuracy= 0.9698\n",
      "Epoch:  24   =====> Loss= 0.113820004  Training accuracy= 0.9708\n",
      "Epoch:  25   =====> Loss= 0.107736760  Training accuracy= 0.9732\n",
      "Epoch:  26   =====> Loss= 0.104203921  Training accuracy= 0.9738\n",
      "Epoch:  27   =====> Loss= 0.101055182  Training accuracy= 0.9726\n",
      "Epoch:  28   =====> Loss= 0.095074212  Training accuracy= 0.9752\n",
      "Epoch:  29   =====> Loss= 0.093346751  Training accuracy= 0.9758\n",
      "Epoch:  30   =====> Loss= 0.089178306  Training accuracy= 0.9776\n",
      "Epoch:  31   =====> Loss= 0.083600454  Training accuracy= 0.9768\n",
      "Epoch:  32   =====> Loss= 0.083257995  Training accuracy= 0.9762\n",
      "Epoch:  33   =====> Loss= 0.081908695  Training accuracy= 0.978\n",
      "Epoch:  34   =====> Loss= 0.075801851  Training accuracy= 0.9782\n",
      "Epoch:  35   =====> Loss= 0.078123385  Training accuracy= 0.9792\n",
      "Epoch:  36   =====> Loss= 0.073123748  Training accuracy= 0.9794\n",
      "Epoch:  37   =====> Loss= 0.072830196  Training accuracy= 0.98\n",
      "Epoch:  38   =====> Loss= 0.068207548  Training accuracy= 0.9798\n",
      "Epoch:  39   =====> Loss= 0.069534003  Training accuracy= 0.9802\n",
      "Epoch:  40   =====> Loss= 0.067791333  Training accuracy= 0.9802\n",
      "Epoch:  41   =====> Loss= 0.065446958  Training accuracy= 0.9812\n",
      "Epoch:  42   =====> Loss= 0.063427341  Training accuracy= 0.9812\n",
      "Epoch:  43   =====> Loss= 0.062668524  Training accuracy= 0.981\n",
      "Epoch:  44   =====> Loss= 0.060850622  Training accuracy= 0.982\n",
      "Epoch:  45   =====> Loss= 0.060145860  Training accuracy= 0.9806\n",
      "Epoch:  46   =====> Loss= 0.058726409  Training accuracy= 0.9828\n",
      "Epoch:  47   =====> Loss= 0.058478678  Training accuracy= 0.9838\n",
      "Epoch:  48   =====> Loss= 0.054889722  Training accuracy= 0.984\n",
      "Epoch:  49   =====> Loss= 0.056067859  Training accuracy= 0.9846\n",
      "Epoch:  50   =====> Loss= 0.053832302  Training accuracy= 0.9842\n",
      "Epoch:  51   =====> Loss= 0.052474880  Training accuracy= 0.9842\n",
      "Epoch:  52   =====> Loss= 0.052806023  Training accuracy= 0.9842\n",
      "Epoch:  53   =====> Loss= 0.051511036  Training accuracy= 0.9844\n",
      "Epoch:  54   =====> Loss= 0.052924352  Training accuracy= 0.9846\n",
      "Epoch:  55   =====> Loss= 0.048252896  Training accuracy= 0.9854\n",
      "Epoch:  56   =====> Loss= 0.047703774  Training accuracy= 0.985\n",
      "Epoch:  57   =====> Loss= 0.048900753  Training accuracy= 0.9846\n",
      "Epoch:  58   =====> Loss= 0.049215919  Training accuracy= 0.9856\n",
      "Epoch:  59   =====> Loss= 0.045154118  Training accuracy= 0.9858\n",
      "Epoch:  60   =====> Loss= 0.046869542  Training accuracy= 0.9864\n",
      "Epoch:  61   =====> Loss= 0.044546843  Training accuracy= 0.9856\n",
      "Epoch:  62   =====> Loss= 0.046014042  Training accuracy= 0.9858\n",
      "Epoch:  63   =====> Loss= 0.043440244  Training accuracy= 0.9862\n",
      "Epoch:  64   =====> Loss= 0.042551775  Training accuracy= 0.9866\n",
      "Epoch:  65   =====> Loss= 0.043002308  Training accuracy= 0.9868\n",
      "Epoch:  66   =====> Loss= 0.042260626  Training accuracy= 0.9862\n",
      "Epoch:  67   =====> Loss= 0.042129999  Training accuracy= 0.9876\n",
      "Epoch:  68   =====> Loss= 0.040326711  Training accuracy= 0.987\n",
      "Epoch:  69   =====> Loss= 0.040119954  Training accuracy= 0.985\n",
      "Epoch:  70   =====> Loss= 0.039372511  Training accuracy= 0.9876\n",
      "Epoch:  71   =====> Loss= 0.039312358  Training accuracy= 0.9866\n",
      "Epoch:  72   =====> Loss= 0.039121126  Training accuracy= 0.9872\n",
      "Epoch:  73   =====> Loss= 0.037745184  Training accuracy= 0.9872\n",
      "Epoch:  74   =====> Loss= 0.038840430  Training accuracy= 0.9878\n",
      "Epoch:  75   =====> Loss= 0.036722533  Training accuracy= 0.9878\n",
      "Epoch:  76   =====> Loss= 0.036708925  Training accuracy= 0.9876\n",
      "Epoch:  77   =====> Loss= 0.035114563  Training accuracy= 0.9876\n",
      "Epoch:  78   =====> Loss= 0.035569670  Training accuracy= 0.9874\n",
      "Epoch:  79   =====> Loss= 0.036551392  Training accuracy= 0.9866\n",
      "Epoch:  80   =====> Loss= 0.034503591  Training accuracy= 0.9882\n",
      "Epoch:  81   =====> Loss= 0.035162572  Training accuracy= 0.9872\n",
      "Epoch:  82   =====> Loss= 0.034061648  Training accuracy= 0.989\n",
      "Epoch:  83   =====> Loss= 0.033039405  Training accuracy= 0.9884\n",
      "Epoch:  84   =====> Loss= 0.032714885  Training accuracy= 0.9896\n",
      "Epoch:  85   =====> Loss= 0.032941225  Training accuracy= 0.9884\n",
      "Epoch:  86   =====> Loss= 0.032947274  Training accuracy= 0.9884\n",
      "Epoch:  87   =====> Loss= 0.032819327  Training accuracy= 0.9876\n",
      "Epoch:  88   =====> Loss= 0.030724695  Training accuracy= 0.9876\n",
      "Epoch:  89   =====> Loss= 0.032069562  Training accuracy= 0.989\n",
      "Epoch:  90   =====> Loss= 0.030840880  Training accuracy= 0.9888\n",
      "Epoch:  91   =====> Loss= 0.031070476  Training accuracy= 0.9884\n",
      "Epoch:  92   =====> Loss= 0.029719276  Training accuracy= 0.989\n",
      "Epoch:  93   =====> Loss= 0.028755416  Training accuracy= 0.9888\n",
      "Epoch:  94   =====> Loss= 0.029784064  Training accuracy= 0.988\n",
      "Epoch:  95   =====> Loss= 0.029574275  Training accuracy= 0.9892\n",
      "Epoch:  96   =====> Loss= 0.028761383  Training accuracy= 0.9878\n",
      "Epoch:  97   =====> Loss= 0.029708285  Training accuracy= 0.9886\n",
      "Epoch:  98   =====> Loss= 0.027081880  Training accuracy= 0.9894\n",
      "Epoch:  99   =====> Loss= 0.027781683  Training accuracy= 0.9888\n",
      "Epoch:  100   =====> Loss= 0.027897487  Training accuracy= 0.9892\n",
      "Epoch:  101   =====> Loss= 0.027332574  Training accuracy= 0.9884\n",
      "Epoch:  102   =====> Loss= 0.026826039  Training accuracy= 0.9888\n",
      "Epoch:  103   =====> Loss= 0.027089774  Training accuracy= 0.9892\n",
      "Epoch:  104   =====> Loss= 0.026156962  Training accuracy= 0.9878\n",
      "Epoch:  105   =====> Loss= 0.025994058  Training accuracy= 0.9884\n",
      "Epoch:  106   =====> Loss= 0.027046072  Training accuracy= 0.9886\n",
      "Epoch:  107   =====> Loss= 0.025342953  Training accuracy= 0.9894\n",
      "Epoch:  108   =====> Loss= 0.025065694  Training accuracy= 0.989\n",
      "Epoch:  109   =====> Loss= 0.025347810  Training accuracy= 0.9886\n",
      "Epoch:  110   =====> Loss= 0.024419306  Training accuracy= 0.9898\n",
      "Epoch:  111   =====> Loss= 0.025100186  Training accuracy= 0.9892\n",
      "Epoch:  112   =====> Loss= 0.024427244  Training accuracy= 0.9882\n",
      "Epoch:  113   =====> Loss= 0.024242153  Training accuracy= 0.989\n",
      "Epoch:  114   =====> Loss= 0.023390765  Training accuracy= 0.988\n",
      "Epoch:  115   =====> Loss= 0.023229957  Training accuracy= 0.989\n",
      "Epoch:  116   =====> Loss= 0.023859585  Training accuracy= 0.9886\n",
      "Epoch:  117   =====> Loss= 0.022953841  Training accuracy= 0.989\n",
      "Epoch:  118   =====> Loss= 0.023012271  Training accuracy= 0.988\n",
      "Epoch:  119   =====> Loss= 0.022594612  Training accuracy= 0.9888\n",
      "Epoch:  120   =====> Loss= 0.020973261  Training accuracy= 0.9892\n",
      "Epoch:  121   =====> Loss= 0.022866992  Training accuracy= 0.989\n",
      "Epoch:  122   =====> Loss= 0.022125616  Training accuracy= 0.9902\n",
      "Accuracy over 99% reached after 122 epochs\n",
      "Training Finished!\n",
      "Test accuracy: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.99019998, 0.98820001)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(learning_rate, training_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.6 </b> : Use tensorBoard to visualise and save the LeNet5 Graph and all learning curves. \n",
    "Save all obtained figures in the folder **\"TP2/MNIST_99_Challenge_Figures\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#insert your obtained figure here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2 </b> : LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> Question 2.2.1 </b>  Change the sigmoid function with a ReLU :\n",
    "\n",
    "- Retrain your network with SGD and AdamOptimizer and then fill the table above  :\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent         |AdamOptimizer |\n",
    "| -------------        |: -------------: | ---------:   \n",
    "| Validation Accuracy  |         |    |      \n",
    "| Testing Accuracy     |           |    |       \n",
    "| Training Time        |           |        |  |  \n",
    "\n",
    "\n",
    "- Try with different learning rates for each Optimizer (0.0001 and 0.001 ) and different Batch sizes (50 and 128) for 20000 Epochs. \n",
    "\n",
    "- For each optimizer, plot (on the same curve) the **testing accuracies** function to **(learning rate, batch size)** \n",
    "\n",
    "\n",
    "\n",
    "- Did you reach the 99% accuracy ? What are the optimal parametres that gave you the best results? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001 Batch size: 50 optimizer: SGC\n",
      "\t====> Time: 4450.806636333466 Validation accuracy: 0.1126 Test accuracy: 0.1135\n",
      "Learning rate: 0.001 Batch size: 50 optimizer: ReLU\n",
      "\t====> Time: 360.2967257499695 Validation accuracy: 0.99 Test accuracy: 0.9858\n",
      "Learning rate: 0.001 Batch size: 128 optimizer: SGC\n",
      "\t====> Time: 3697.093007326126 Validation accuracy: 0.1126 Test accuracy: 0.1135\n",
      "Learning rate: 0.001 Batch size: 128 optimizer: ReLU\n",
      "\t====> Time: 696.3485853672028 Validation accuracy: 0.9906 Test accuracy: 0.9876\n",
      "Learning rate: 0.0001 Batch size: 50 optimizer: SGC\n",
      "\t====> Time: 4444.108605384827 Validation accuracy: 0.1126 Test accuracy: 0.1135\n",
      "Learning rate: 0.0001 Batch size: 50 optimizer: ReLU\n",
      "\t====> Time: 4500.74076128006 Validation accuracy: 0.989 Test accuracy: 0.9877\n",
      "Learning rate: 0.0001 Batch size: 128 optimizer: SGC\n",
      "\t====> Time: 3724.1400146484375 Validation accuracy: 0.1126 Test accuracy: 0.1135\n",
      "Learning rate: 0.0001 Batch size: 128 optimizer: ReLU\n",
      "\t====> Time: 3732.6566574573517 Validation accuracy: 0.9876 Test accuracy: 0.9881\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "learning_rates = [0.001, 0.0001]\n",
    "batch_sizes = [50, 128]\n",
    "optNames = [\"SGC\", \"ReLU\"]\n",
    "training_epochs = 150\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        for on in optNames:\n",
    "            print(\"Learning rate:\", lr, \"Batch size:\", bs, \"optimizer:\", on)\n",
    "            t1 = time.time()\n",
    "            val_acc, test_acc = train(learning_rate=lr, training_epochs=training_epochs, batch_size=bs, \\\n",
    "                                      display_step=50, optFunction=on, verbose=False)\n",
    "            t2 = time.time() - t1\n",
    "            print(\"\\t====> Time:\", t2, \"Validation accuracy:\", val_acc, \"Test accuracy:\", test_acc)\n",
    "            results.append((lr, bs, on, t2, val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    (0.001, 50, \"SGC\", 4450.806636333466, 0.1126, 0.1135),\n",
    "    (0.001, 50, \"ReLU\", 360.2967257499695, 0.99, 0.9858),\n",
    "    (0.001, 128, \"SGC\", 3697.093007326126, 0.1126, 0.1135),\n",
    "    (0.001, 128, \"ReLU\", 696.3485853672028, 0.9906, 0.9876),\n",
    "    (0.0001, 50, \"SGC\", 4444.108605384827, 0.1126, 0.1135),\n",
    "    (0.0001, 50, \"ReLU\", 4500.74076128006, 0.989, 0.9877),\n",
    "    (0.0001, 128, \"SGC\", 3724.1400146484375, 0.1126, 0.1135),\n",
    "    (0.0001, 128, \"ReLU\", 3732.6566574573517, 0.9876, 0.9881)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJcAAAJCCAYAAAB9KiZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe8304b4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot([x[0] for x in results if x[2] is \"ReLU\"], [x[4] for x in results  if x[2] is \"ReLU\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.2 </b>  What about applying a dropout layer on the Fully connected layer and then retraining the model with the best Optimizer and parameters(Learning rate and Batch size) obtained in *Question 2.2.1*  ? (probability to keep units=0.75). For this stage ensure that the keep prob is set to 1.0 to evaluate the \n",
    "performance of the network including all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your implementaion goas here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Your comments go here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
